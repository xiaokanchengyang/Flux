#!/bin/bash
# Cloud Storage Examples for Flux
# This script demonstrates how to use Flux with cloud storage providers

echo "=== Flux Cloud Storage Examples ==="
echo ""
echo "Prerequisites:"
echo "1. Build flux with cloud support: cargo build --release --features cloud"
echo "2. Set up cloud credentials (see below)"
echo ""

echo "=== Amazon S3 ==="
echo "Required environment variables:"
echo "  export AWS_ACCESS_KEY_ID=your_access_key"
echo "  export AWS_SECRET_ACCESS_KEY=your_secret_key"
echo "  export AWS_REGION=us-east-1  # optional, defaults to us-east-1"
echo ""
echo "Examples:"
echo "  # Pack a directory to S3:"
echo "  flux pack -i /path/to/data -o s3://my-bucket/backups/data.tar.zst"
echo ""
echo "  # Extract from S3:"
echo "  flux extract s3://my-bucket/backups/data.tar.zst -o ./restored-data"
echo ""
echo "  # Inspect archive in S3:"
echo "  flux inspect s3://my-bucket/backups/data.tar.zst"
echo ""

echo "=== Google Cloud Storage ==="
echo "Required environment variables:"
echo "  export GOOGLE_APPLICATION_CREDENTIALS=/path/to/service-account-key.json"
echo "  # OR"
echo "  export GOOGLE_SERVICE_ACCOUNT='{\"type\": \"service_account\", ...}'"
echo ""
echo "Examples:"
echo "  # Pack to GCS:"
echo "  flux pack -i /path/to/data -o gs://my-bucket/backups/data.tar.gz"
echo ""
echo "  # Extract from GCS:"
echo "  flux extract gs://my-bucket/backups/data.tar.gz -o ./restored-data"
echo ""

echo "=== Azure Blob Storage ==="
echo "Required environment variables:"
echo "  export AZURE_STORAGE_ACCOUNT_NAME=myaccount"
echo "  export AZURE_STORAGE_ACCOUNT_KEY=your_account_key"
echo "  # OR use SAS token:"
echo "  export AZURE_STORAGE_SAS_TOKEN=your_sas_token"
echo ""
echo "Examples:"
echo "  # Pack to Azure:"
echo "  flux pack -i /path/to/data -o az://my-container/backups/data.tar.xz"
echo ""
echo "  # Extract from Azure:"
echo "  flux extract az://my-container/backups/data.tar.xz -o ./restored-data"
echo ""

echo "=== Advanced Usage ==="
echo ""
echo "1. Smart compression with cloud output:"
echo "   flux pack -i /var/log --smart -o s3://logs-bucket/server-logs.tar"
echo ""
echo "2. Interactive extraction from cloud:"
echo "   flux extract s3://backups/archive.tar.gz -o ./output --interactive"
echo ""
echo "3. Specify compression algorithm and level:"
echo "   flux pack -i ./data -o gs://bucket/data.tar --algo zstd --level 19"
echo ""
echo "4. Extract with path stripping:"
echo "   flux extract s3://bucket/archive.tar.gz --strip-components 2 -o ./output"
echo ""

echo "=== Performance Tips ==="
echo "1. flux-cloud uses 8MB buffers by default for efficient streaming"
echo "2. Large files automatically use multipart upload (16MB+ files)"
echo "3. Cloud operations are optimized to minimize API calls"
echo "4. Consider using --smart flag to choose optimal compression for your data"
echo ""

echo "=== Troubleshooting ==="
echo "1. Check credentials: flux will verify credentials before starting"
echo "2. Network errors: flux will report detailed error messages"
echo "3. For debugging, use: RUST_LOG=flux=debug flux [command]"
echo ""